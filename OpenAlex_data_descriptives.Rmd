---
title: "Untitled"
author: "Ana Macanovic"
date: "2024-01-11"
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(eval = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

Load the packages:
```{r message=  F, warning = F}
library(groundhog)
packages_to_load <- c("readr", "dplyr", 
                      "stringr", "lubridate",
                      "tidyr", "tidyverse")
groundhog.library(packages_to_load, date = "2023-12-01")

```

# Load helpful data

Load the professor profiles:
```{r message = F, warning = F}
load("raw_data/media_profs_profiles.rda")
profs <- read_csv("raw_data/dutch_profs_urls.csv")

# merge the profs with their ORCIDs
colnames(profs)[c(1,7)] <- c("id", "profile_id")

profs_full <- merge(profs,
                    metadf[, c(1:4, 361)],
                    by = "profile_id")
```

Load the professor gender inference:
```{r}
profs_full_gender <- readRDS("~/Postdoc/Projects/dutch_media_profs_r/processed_data/profs_full_gender.RDS")
```

And grant info:
```{r}
nwo_grants <- readRDS("~/Postdoc/Projects/dutch_media_profs_r/processed_data/nwo_grants.RDS")
```


# Extract URLs

Extra the URLs from news mentions of each professor:
```{r}
# stop dplyr from informing us about every little thing
options(dplyr.summarise.inform = FALSE)

# query in batches
narcis_ids <- profs_full$profile_id
# batch size
prof_batch_size <- 500
# vector of indices to loop through
batches <- seq(from=1, to=length(narcis_ids), by=prof_batch_size)
# to be able to subset, also add the final index+1
batches <- c(batches, length(narcis_ids)+1)

# dataframe to store urls we want to scrape
url_data <- data.frame(matrix(NA, nrow = 0, ncol = 5))


# loop through the batches
for(i in 1:(length(batches)-1)){
  # read in the first batch
  prof_batch <- readRDS(paste0("processed_data/open_alex_prof_data/", paste("prof_data_batch", i, sep = "_"), ".RDS"))
  
  # initialize a new list
  prof_yearly_info_batch <- list()
  
  # get the data for cumulative info
  prof_cumulative_info_dataframe_batch <- data.frame(matrix(NA, ncol = 0, nrow = 0))
  
  # loop through profs in the list
  for (j in 1:length(prof_batch)){
    prof_yearly <- list()
    prof <- prof_batch[[j]]
    # if some data in the prof list
    if (length(prof)>1){
      # get the prof_id and other info
      prof_id <- names(prof_batch[j])
      prof_yearly['profile_id'] <- prof_id
      prof_yearly["oa_ids"] <- list(prof[["oa_ids"]])
      prof_yearly["orcid"] <- prof[["orcid"]]
      prof_yearly["concept_info"] <- NA
      
      # get the inferred gender
      prof_yearly["gender"] <- filter(profs_full_gender, profile_id == prof_id)[, 'inferred_gender']
      
      # get their NWO Grants
      prof_yearly["nwo_grants"] <- NA
      prof_nwo_grants <- filter(nwo_grants, profile_id == prof_id)
      if (!all(is.na(prof_nwo_grants))){
      prof_yearly["nwo_grants"]<- list(prof_nwo_grants)
      }
      # get the publications
      oa_pubs <- prof[['oa_pubs']]
      prof_yearly['oa_pub_info'] <- NA
      # if any OA pubs, get the yearly breakdown of pubs, their citations up until
      # 2012, and their mentions
      if (! all(is.na(oa_pubs))){
        # for each publication
        pub_info_year_all <- data.frame(matrix(NA, ncol = 0, nrow = 0))
        pub_concept_info <- data.frame(matrix(NA, ncol = 0, nrow = 0))
        for (k in 1:nrow(oa_pubs)){
          # get relevant row
          pub <- oa_pubs[k, ]
          # get the relevant info
          pub_info <- oa_pubs[k, c("id", "display_name",
                                   "author", "author_position",
                                   "so", "so_id", "doi", "language", 
                                   "grants", "cited_by_count", "publication_year",
                                   "cited_by_api_url", "type", "profile_id", "concepts")]
          # get professor concepts
          concept_pub <- unnest(pub_info[, "concepts"], cols = c(concepts))
          if (!all(is.na(concept_pub))){
          # leave only 0 and 1 levels, with confidence of at least 50%
          concept_pub <- filter(concept_pub, level <= 1 & score >= 0.5)
          # colnames
          colnames(concept_pub) <- c("concept_id", "concept_wikidata", "concept_display_name",
                                     "level", "score")
          # merge with pub_id and year
          concept_pub_add <- bind_cols(pub_info[, c("id", "publication_year")],
                                       concept_pub)
          
          # add in with other concept info
          pub_concept_info <- bind_rows(pub_concept_info,
                                        concept_pub_add)
          }
          
          # get citation counts by year
          citations_year <- data.frame(pub$counts_by_year)
          
          # if not empty
          if (!all(is.na(citations_year))){
            colnames(citations_year)[2] <- "cited_by_year"
            
            # check if any mentions
            if (which(colnames(pub) == "author_position") == ncol(pub)){
              pub_info_year <- bind_cols(pub_info, citations_year)
              pub_info_year_all <- bind_rows(pub_info_year_all,
                                             pub_info_year)
            }else{
            # get all the cols after author position  for mentions
            pub_mentions <- pub[, c((which(colnames(pub) == "author_position")+1):ncol(pub))]
            
            # and now loop through the mentions
            mentions_per_year_all <- data.frame(matrix(NA, ncol = 0, nrow = 0))
            for (col in colnames(pub_mentions)){
              col_data <- unnest(subset(pub_mentions, select=col), cols = col)
              if (!all(is.na(col_data))){
                col_data$year <- year(ymd_hms(col_data$posted_on))
                # compile per year
                mentions_per_year <- col_data %>%
                  group_by(year)%>%
                  summarise(n = n())
                colnames(mentions_per_year)[which(colnames(mentions_per_year) == "n")] <- paste0("n_", col)
                
                # add on
                if ("year" %in% colnames(mentions_per_year_all)){
                  mentions_per_year_all <- merge(mentions_per_year,
                                                 mentions_per_year_all,
                                                 all.x = TRUE,
                                                 all.y = TRUE,
                                                 by = "year")
                }else{
                  mentions_per_year_all <- mentions_per_year
                }
              }
            }
            
            # merge citations and mentions if any mentions
            if (!all(is.na(mentions_per_year_all))){
              citations_mentions_year <- merge(citations_year,
                                               mentions_per_year_all,
                                               by = "year",
                                               all.x = TRUE,
                                               all.y = TRUE)
            }else{
              citations_mentions_year <- citations_year
            }
            
            # and now merge this info with the rest of publication info
            pub_info_year <- bind_cols(pub_info, citations_mentions_year)
            pub_info_year_all <- bind_rows(pub_info_year_all,
                                           pub_info_year)
          }
          }
        }
        
        prof_yearly['oa_pub_info'] <- list(pub_info_year_all)
        
        # and now get cumulative output if any publication info
        prof_yearly['oa_pub_info_cumulative'] <- NA
        oa_pub_info_cumulative <- NA
        if (!all(is.na(pub_info_year_all))){
        # accumulate the pub info per year
        # replace NAs with zeroes
        pub_info_year_all_fill <- pub_info_year_all
        pub_info_year_all_fill[, (which(colnames(pub_info_year_all_fill)=="year")+1):ncol(pub_info_year_all_fill)][is.na(pub_info_year_all_fill[, (which(colnames(pub_info_year_all_fill)=="year")+1):ncol(pub_info_year_all_fill)])]  <- 0
        
        # get the sums of their citations + mentions that year, since 2011
        oa_pub_info_cumulative <- pub_info_year_all_fill %>%
          filter(year > 2011)%>%
          select(year:last_col())%>%
          group_by(year)%>%
          summarise_all(list(sum))
        
        # get the sums of their newly published papers since 2011
        oa_pub_publication_year <- pub_info_year_all_fill %>%
          filter(publication_year > 2011)%>%
          group_by(publication_year)%>%
          summarise(pubs = n())
        
        colnames(oa_pub_publication_year)[1] <- "year"
        # merge these two sources of info
        oa_pub_info_cumulative <- merge(oa_pub_info_cumulative,
                                        oa_pub_publication_year,
                                        all.x = TRUE,
                                        all.y = TRUE)
        
        # replace NAs with zeroes
        oa_pub_info_cumulative[is.na(oa_pub_info_cumulative)]  <- 0
        
        prof_yearly['oa_pub_info_cumulative'] <- list(oa_pub_info_cumulative)
        }
      }
      
      if (!all(is.na(oa_pub_info_cumulative))){
        cumulative_info_row <- bind_cols(profile_id = prof_id, oa_pub_info_cumulative)
        # add in the publication/mention info
        prof_cumulative_info_dataframe_batch <- bind_rows(prof_cumulative_info_dataframe_batch,
                                                          cumulative_info_row)
      }
      
      # add in the grant and coauthorship info
      prof_yearly["grant_info"] <- list(prof[["grant_info"]])
      prof_yearly["affiliation_info"] <- list(prof[["affiliation_info"]])
      prof_yearly["coauthor_info"] <- list(prof[["coauthor_info"]])
      prof_yearly["prof_info"] <-  list(prof[["prof_info"]])
      
      # and get the author's most prominent concepts per year, keeping those with the
      if (!all(is.na(pub_concept_info))){
        # average yearly scores above 0.5
        prominent_concepts <- pub_concept_info %>% 
        group_by(publication_year, level, concept_display_name, id, concept_id, concept_wikidata)%>%
        summarise(avg_confidence_score = mean(score))%>%
        filter(., avg_confidence_score >= 0.5)
      
      prof_yearly["concept_info"] <-  list(prominent_concepts)
      }
    }
      prof_yearly_info_batch[[prof_id]] <- prof_yearly
    
    print(paste("done with", j, "out of", length(prof_batch)))
  }
  
    
  # save the batch RDS
  save_string <- paste0("processed_data/open_alex_prof_yearly_data/", paste("prof_data_batch", i, sep = "_"), ".RDS")
  saveRDS(prof_yearly_info_batch, save_string)
  
  # save the cumulative info
  prof_cumulative_info_dataframe_batch[is.na(prof_cumulative_info_dataframe_batch)] <- 0
  save_string2 <- paste0("processed_data/open_alex_prof_yearly_data/", paste("cumulative_info_batch", i, sep = "_"), ".RDS")
  saveRDS(prof_cumulative_info_dataframe_batch, save_string2)
  
  print(paste("done with batch", i))
}
```

# Descriptives of our data

```{r}
# getting all the csv files that were processed in batches
cumulative_info <- list.files(path = "processed_data/open_alex_prof_yearly_data/",
                              pattern = "cumulative",
                              full.names = TRUE)%>%
  map(readRDS) %>% 
  bind_rows()

```

Get some averages per year:
```{r}
year_averages <- cumulative_info %>%
  group_by(year)%>% 
  summarise(n = n())
```

