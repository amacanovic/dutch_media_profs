---
title: "Nexis_data_handling"
author: "Ana Macanovic"
date: "2023-12-06"
---

This script reads in and parses printed news data from LexisNexis.


Load the packages:
```{r eval = T, echo=FALSE, results='hide', message = F, warning=F}
source("helper_functions.R")
packages_to_load <- c("readr", "dplyr", "stringr", "tidyr", "tidyverse",
                      "RPostgres", "lubridate", "readtext", "knitr",
                      "DBI", "RODBC", "odbc")
fpackage_check(packages_to_load)

# For full reproducibility, load the packages with groundhog using the code below instead
# of the fpackage_check function

# library(groundhog)
# groundhog.library(packages_to_load, date = "2023-12-01")
```



```{r}
opts_chunk$set(echo = TRUE)
opts_chunk$set(eval = FALSE)
opts_chunk$set(warning = FALSE)
opts_chunk$set(message = FALSE)
```

Load the R package by J. Grubber that conveniently tidies up the LexisNexis data:
```{r message=  F, warning = F}
# remotes::install_github("JBGruber/LexisNexisTools")
library("LexisNexisTools")
```

# Tidy up professor data from NARCIS

Connect to database:
```{r}
# default port here, change if needed
port <- 5432
# username
user <- ""
# password
password <- ""
# database name
database_name <- ""

con <- dbConnect(Postgres(),
                 dbname= database_name,
                 port = port,
                 user = user, 
                 password = password)

```

# Loading in the data

Script "lexis_helper_compile.R" compiles data from various download sub-folders, 
cleans it, and removes professors we do not consider due to ambiguous names.

Below, we leave an example of the code we use to extract the data from files
downloaded from Lexis Uni.

Load the word files from the individual professor zips and extract the 
article texts:
```{r}
folder <- "../downloads/zipfiles"
all_docs <- lnt_read(x = folder, convert_date = F, extract_paragraphs = F)
meta_data <- all_docs@meta
meta_data$profile_id <- str_split_i(meta_data$Source_File, "/zips/", 2)
meta_data$profile_id <- str_split_i(meta_data$profile_id, "_", 1)
```

# Clean the LexisNexis data

Tidy up the dates:
```{r}
meta_data$date_eng <- tolower(meta_data$Date)
# replace dutch dates with english for easier conversion later on
meta_data$date_eng <- str_replace(meta_data$date_eng,
                                  "januari",
                                  "january")
meta_data$date_eng <- str_replace(meta_data$date_eng,
                                  "februari",
                                  "february")
meta_data$date_eng <- str_replace(meta_data$date_eng,
                                  "maart",
                                  "march")
meta_data$date_eng <- str_replace(meta_data$date_eng,
                                  "mei",
                                  "may")
meta_data$date_eng <- str_replace(meta_data$date_eng,
                                  "mai",
                                  "may")
meta_data$date_eng <- str_replace(meta_data$date_eng,
                                  "juni",
                                  "june")
meta_data$date_eng <- str_replace(meta_data$date_eng,
                                  "juli",
                                  "july")
meta_data$date_eng <- str_replace(meta_data$date_eng,
                                  "augustus",
                                  "august")
meta_data$date_eng <- str_replace(meta_data$date_eng,
                                  "oktober",
                                  "october")
meta_data$date_eng <- str_replace(meta_data$date_eng,
                                  "dezember",
                                  "december")

# convert the dates
meta_data$date_tidy <- parse_date_time(meta_data$date_eng,  c("dmy", "md,y"))

# get the year
meta_data$year <- year(meta_data$date_tidy)

# select columns of interest
meta_data_sel <- meta_data %>%
  select(ID, Newspaper, Length:Author, Headline, profile_id, date_tidy, year)

colnames(meta_data_sel) <- tolower(colnames(meta_data_sel))
```

Prepare the article texts:
```{r}
article_content <- all_docs@articles
colnames(article_content) <- tolower(colnames(article_content))
```

Clean up the memory:
```{r}
rm(all_docs)
rm(missing_indices)
gc()
```

Merge the metadata and the content
```{r}
news_mentions <- merge(meta_data_sel,
                       article_content,
                       by = "id")

# profile ID to match the rest
news_mentions$profile_id <- paste0("https://www.narcis.nl/person/RecordID/", news_mentions$profile_id)
```

Filter our professors we do not want to consider due to ambiguous names:
```{r}
exclude_profs <- c('PRS1260654', 'PRS1264232', 'PRS1290223',
                   'PRS1290912','PRS1291282', 'PRS1298775',
                   'PRS1299517', 'PRS1303190', 'PRS1308364',
                   'PRS1313821', 'PRS1314292', 'PRS1315919',
                   'PRS1316094', 'PRS1321926', 'PRS1324504',
                   'PRS1325131', 'PRS1329040', 'PRS1330089',
                   'PRS1331627', 'PRS1331980', 'PRS1332877',
                   'PRS1334007', 'PRS1338934', 'PRS1341238',
                   'PRS1349009', 'PRS1350774', 'PRS1260039',
                   'PRS1265665', 'PRS1276211', 'PRS1336203',
                   'PRS1329967', 'PRS1334028')
```


# Article classification
Denote whether an article is from a national, regional or specialized Dutch source, or perhaps a higher-profile international source:
```{r}
source("resources/lexis_news_outlet_classification.R")
```


Remove spaces around / for easier identification:
```{r}
news_mentions$newspaper_tidy <- str_replace(news_mentions$newspaper, " \\/ ", "\\/")
```

Denote whether the article belongs to any of these, and then also exclude
one source that works a lot with newsletters, but not actual news:
```{r}
news_mentions$source_type <- NA

# first, Dutch sources
news_mentions$source_type <- ifelse(tolower(news_mentions$newspaper_tidy) %in% tolower(national_news_nl),
                                    "national_nl", 
                                    news_mentions$source_type)

news_mentions$source_type <- ifelse(tolower(news_mentions$newspaper_tidy) %in% tolower(regional_news_nl),
                                    "regional_nl", 
                                    news_mentions$source_type)

# International sources
news_mentions$source_type <- ifelse(tolower(news_mentions$newspaper_tidy) %in% tolower(high_profile_int),
                                    "high_prof_intl", 
                                    news_mentions$source_type)

news_mentions$source_type <- ifelse(tolower(news_mentions$newspaper_tidy) %in% tolower(local_int),
                                    "local_int", 
                                    news_mentions$source_type)


news_mentions$source_type <- ifelse(tolower(news_mentions$newspaper_tidy) %in% tolower(other_int),
                                    "other_int", 
                                    news_mentions$source_type)

# News aggregators
news_mentions$source_type <- ifelse(tolower(news_mentions$newspaper_tidy) %in% tolower(aggregators),
                                    "news_aggr", 
                                    news_mentions$source_type)

# Specialized outlets
news_mentions$source_type <- ifelse(tolower(news_mentions$newspaper_tidy) %in% tolower(finance_news),
                                    "finance", 
                                    news_mentions$source_type)

news_mentions$source_type <- ifelse(tolower(news_mentions$newspaper_tidy) %in% tolower(professional_pub),
                                    "prof", 
                                    news_mentions$source_type)


news_mentions$source_type <- ifelse(tolower(news_mentions$newspaper_tidy) %in% tolower(science_pub),
                                    "science", 
                                    news_mentions$source_type)

# Blogs
news_mentions$source_type <- ifelse(str_detect(tolower(news_mentions$newspaper_tidy), "blog"), 
                             "blog", 
                             news_mentions$source_type)

# No source
news_mentions$source_type <- ifelse(news_mentions$newspaper_tidy == "" | is.na(is.na(news_mentions$newspaper_tidy)),
                                    "unknown", 
                                    news_mentions$source_type)

# All the rest are classified as "other" (66k)
news_mentions$source_type <- ifelse(is.na(news_mentions$source_type),
                                    "other",
                                    news_mentions$source_type)
```

See if the source is an online outlet/website:
```{r}
news_mentions$online_resource <- str_detect(tolower(news_mentions$newspaper_tidy), "\\.nl")
news_mentions$online_resource_2 <- str_detect(tolower(news_mentions$newspaper_tidy), "\\.co")
news_mentions$online_resource_3 <- str_detect(tolower(news_mentions$newspaper_tidy), "online")
news_mentions$online_resource <- ifelse(news_mentions$online_resource == TRUE | news_mentions$online_resource_2 == TRUE |
                                          news_mentions$online_resource_3 == TRUE ,
                                        1,
                                        0)

news_mentions <- news_mentions %>% select(-online_resource_2, -online_resource_3)
```


## See mentions of professor's names AND institutions

For robustness checks, we might want to know which articles include both the
professor's last name and any of the institutions they have been affiliated with
during their lifetimes. This could help us exclude articles about people with the
same name and some sort of academic/university affiliation, but who are not our
professor in question. 

Read in the institutional search:
```{r}
all_prof_inst_search <- dbReadTable(con, "oa_affiliation_string_search")
```


Now, merge the professor IDs with Lexis outputs:
```{r}
colnames(all_prof_inst_search)[1] <- "profile_id"
news_mentions_profs <- merge(news_mentions,
                             all_prof_inst_search,
                             by = "profile_id",
                             all.x = TRUE)
```

Now also include their last name:
```{r}
profs_full <- dbReadTable(con, "narcis_prof_info")

news_mentions_profs <- merge(news_mentions_profs,
                             profs_full[c("profile_id", "last")],
                             by = "profile_id")

# lowercase text and uni strings
news_mentions_profs$article_lowercase <- tolower(news_mentions_profs$article)
news_mentions_profs$string_match_names_lower <- tolower(news_mentions_profs$string_match_names)

# remove special characters from the string match columns
news_mentions_profs$string_match_names_lower <- gsub("[\U4E00-\U9FFF\U3000-\U303F]", "", news_mentions_profs$string_match_names_lower)
news_mentions_profs$string_match_names_lower <- gsub("（)", "", news_mentions_profs$string_match_names_lower)
news_mentions_profs$string_match_names_lower <- gsub("\\b\\b|", "", news_mentions_profs$string_match_names_lower)
news_mentions_profs$string_match_names_lower <- gsub("\\)", "", news_mentions_profs$string_match_names_lower)
news_mentions_profs$string_match_names_lower <- gsub("\\(", "", news_mentions_profs$string_match_names_lower)
```

Detect affiliation:
```{r}
news_mentions_profs$affiliation <- str_detect(news_mentions_profs$article_lowercase, news_mentions_profs$string_match_names_lower)
```


Flag mentions to remove multiple regional newspapers discussing the same
person:
```{r}
news_mentions_profs$regional_duplicate <- duplicated(news_mentions_profs[c("profile_id", "headline", "length")])
```

Drop redundant fields:
```{r}
news_mentions_profs <- news_mentions_profs %>%
    select(-string_match_names, -last, -article_lowercase, -string_match_names_lower)
```

Write this out to our database:
```{r}
dbWriteTable(con, "lexis_nexis_mentions", news_mentions_profs)
```


